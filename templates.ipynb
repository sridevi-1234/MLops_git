{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridevi-1234/MLops_git/blob/master/templates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97cd4b2-0127-48ea-8237-641c715f88c9",
      "metadata": {
        "id": "d97cd4b2-0127-48ea-8237-641c715f88c9"
      },
      "source": [
        "# **Templates**\n",
        "**(PromptTemplate and ChatPromptTemplate)**\n",
        "\n",
        "<img src=\"images/langchain_model_io.jpg\">\n",
        "\n",
        "LangChain Expression Language (LCEL) makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.\n",
        "\n",
        "The most basic and common use case is chaining a prompt template and a model together.\n",
        "\n",
        "<img src=\"images/langchain_LCEL.JPG\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd999c2-5a6b-49bb-a0d8-902ec9e72623",
      "metadata": {
        "id": "0cd999c2-5a6b-49bb-a0d8-902ec9e72623"
      },
      "source": [
        "## **PromptTemplate**\n",
        "\n",
        "Input to a model can be a **string** value or **chat message** list.\n",
        "\n",
        "**Prompt Template**  \n",
        "- Prompt Templates are used to convert raw user input to a better input to the LLM.\n",
        "- Templates allow us to easily configure and modify our input prompts to LLM calls.\n",
        "- A template may include instructions, few-shot examples, and specific context and questions appropriate for a given task.\n",
        "- LangChain provides tooling to create and work with prompt templates.\n",
        "- LangChain strives to create model agnostic templates to make it easy to reuse existing templates across different language models.\n",
        "- Typically, language models expect the prompt to either be a string or else a list of chat messages.\n",
        "\n",
        "\n",
        "Prompt templates are used to convert raw user input to a better input to the LLM or ChatModels.  \n",
        "Templates offer a more systematic approach to passing in variables to prompts for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa4357d6-0223-47ad-8d81-666d6f839958",
      "metadata": {
        "id": "aa4357d6-0223-47ad-8d81-666d6f839958"
      },
      "source": [
        "### **PromptTemplate - .format(), .format_messages() and .format_prompt()**\n",
        "\n",
        "Templates offer a more systematic approach to passing in variables to prompts for models, instead of using f-string literals or .format() calls. The PromptTemplate converts these into function parameter names that we can pass in.\n",
        "\n",
        "- .format(): Converts the PromptTemplate to `String`\n",
        "- .format_message(): Converts the PromptTemplate to list of `ChatMessages`\n",
        "- .format_prompt(): Converts the PromptTempate to `StringPromptValue`. `PromptValues` can be converted to both LLM (to string) inputs and ChatModel (to messages) inputs. On this we can apply `to_messages()` or `to_string()`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13ab60b4-68a8-4c0c-864d-4eff3e7c0c29",
      "metadata": {
        "id": "13ab60b4-68a8-4c0c-864d-4eff3e7c0c29"
      },
      "source": [
        "## **ChatPromptTemplate**\n",
        "\n",
        "The prompt to chat models is a list of chat messages.\n",
        "\n",
        "Each chat message is associated with content, and an additional parameter called role. For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}