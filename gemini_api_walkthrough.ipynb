{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridevi-1234/MLops_git/blob/master/gemini_api_walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46f5da68-31b4-4f66-8e3c-de6705eac5c9",
      "metadata": {
        "id": "46f5da68-31b4-4f66-8e3c-de6705eac5c9"
      },
      "source": [
        "# **Introduction to GoogleAI**\n",
        "\n",
        "Launched in December 2023 after PaLM (Pathways Language Model) and LaMDA - Language Model for Dialoge application).\n",
        "\n",
        "You can use Gemini chatbot which was formally known as Bard. Google announced Bard in February 2023 as a GenAI chatbot powered by LaMDA. Later chatbot switched to PaLM model before finally switching to the Gemini model.\n",
        "\n",
        "Let's list down a few reasons as why you might want to choose Gemini.\n",
        "- **Context Window:** In May 2024, Gemini 1.5 was updated with a context window of 2 million tokens. To put that in perspective, 2 million tokens can  process up to 2 hours of video, 22 hours of audio, 60K lines of code, or 1.4 million words of text.\n",
        "- **Multimodal Capabilities:** Works with text, images and videos.\n",
        "- **Variety of options:** Variants: Ultra, Pro, Flash and Nano.\n",
        "- **Generous free offerings:** Offers a free to use option.\n",
        "\n",
        "**Important Links:**\n",
        "1. [Gemini Chatbot](https://gemini.google.com/app)\n",
        "2. []()\n",
        "\n",
        "**Dependencies:**\n",
        "```python\n",
        "! pip install google-generativeai\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51ed763-72b9-4f29-b10e-b75c537597c9",
      "metadata": {
        "id": "d51ed763-72b9-4f29-b10e-b75c537597c9"
      },
      "outputs": [],
      "source": [
        "#!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d1d7b5d-b943-4bab-bbbe-8a176fd5130e",
      "metadata": {
        "id": "9d1d7b5d-b943-4bab-bbbe-8a176fd5130e"
      },
      "source": [
        "## **Importing Google Gemini AI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb7d1b8-f430-4f9b-b2a4-417a8db6313c",
      "metadata": {
        "id": "dfb7d1b8-f430-4f9b-b2a4-417a8db6313c"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3e0b31b-e2db-43e7-9999-347e3c441724",
      "metadata": {
        "id": "b3e0b31b-e2db-43e7-9999-347e3c441724"
      },
      "source": [
        "## **Setting the API Key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad6eea9-585d-427f-aa76-da4d26111804",
      "metadata": {
        "id": "0ad6eea9-585d-427f-aa76-da4d26111804"
      },
      "outputs": [],
      "source": [
        "f = open(\"keys/.gemini.txt\")\n",
        "key = f.read()\n",
        "\n",
        "genai.configure(api_key=key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991360dd-a618-479a-904d-5e07661e2697",
      "metadata": {
        "id": "991360dd-a618-479a-904d-5e07661e2697"
      },
      "source": [
        "## **Available Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2122cd5-8425-435b-b5d8-17efcf5362fc",
      "metadata": {
        "id": "e2122cd5-8425-435b-b5d8-17efcf5362fc",
        "outputId": "055fd630-5d72-43ef-a95a-4b912cab68a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-exp-0801\n",
            "models/gemini-1.5-pro-exp-0827\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-exp-0827\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/aqa\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ef743af-5075-469a-9126-200e13439f7c",
      "metadata": {
        "id": "7ef743af-5075-469a-9126-200e13439f7c"
      },
      "source": [
        "## **Prompting the Gemini Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74ced0d0-98ef-48a5-bb03-8372a4576604",
      "metadata": {
        "id": "74ced0d0-98ef-48a5-bb03-8372a4576604",
        "outputId": "39de47ae-004b-4ea9-8ac2-5b4f3c9f652c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In our solar system, Earth is a **planet**. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initializing the GoogleAI Model\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "\n",
        "# Defining the Prompt\n",
        "user_prompt = \"\"\"Complete the following:\n",
        "                In our solar system, Earth is a \"\"\"\n",
        "\n",
        "# Calling the model with the prompt\n",
        "response = model.generate_content(user_prompt)\n",
        "\n",
        "# Printing the response\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0e6f5b-63c6-4d7d-a488-ed22c6344a77",
      "metadata": {
        "id": "ab0e6f5b-63c6-4d7d-a488-ed22c6344a77",
        "outputId": "f18e4f3c-2fc1-4e24-8832-51e00a98e416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In our solar system, Earth is a **rocky planet**, the **third planet from the Sun**, and the **only known planet to harbor life**. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "user_prompt = \"\"\"Generate some factual information to complete the following in 2-3 lines:\n",
        "                In our solar system, Earth is a \"\"\"\n",
        "\n",
        "response = model.generate_content(user_prompt)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59aba55e-c0a3-416e-8c94-4ac1ffeae39c",
      "metadata": {
        "id": "59aba55e-c0a3-416e-8c94-4ac1ffeae39c"
      },
      "source": [
        "## **Adding a System Prompt**\n",
        "\n",
        "**Important Note:** System Prompt can be specified using `system_instruction`. `system_instruction` is not enabled for models/gemini-pro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b18e562-75f5-4eb0-a312-f7a30fccbec7",
      "metadata": {
        "id": "9b18e562-75f5-4eb0-a312-f7a30fccbec7",
        "outputId": "3dc40d5b-aae9-4b37-da55-2ccdba6552f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...rocky planet that orbits the Sun, and is the only known astronomical object to harbor life. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-pro-latest\",\n",
        "                              system_instruction=\"\"\"Generate some factual information to complete the user input.\n",
        "                              Completion must have maximum 2-3 lines.\"\"\")\n",
        "\n",
        "user_prompt = \"\"\"In our solar system, Earth is a \"\"\"\n",
        "\n",
        "response = model.generate_content(user_prompt)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e1edeb-1b26-471e-a008-f78a5774f545",
      "metadata": {
        "id": "77e1edeb-1b26-471e-a008-f78a5774f545"
      },
      "source": [
        "## **Important Parameters**\n",
        "\n",
        "**Reference - [HuggingFace Blog](https://huggingface.co/blog/how-to-generate)**\n",
        "\n",
        "If you run the above code few times, you will notice that the output changes across runs. Generative models are **non-deterministic**. This means that even with the same input they can produce different outputs. This behavior allows for creativity and diversity in the generated outputs, which can be great when trying to generate different creative styles. There are parameters which can help us control this behavior like temperature, top_p, etc...\n",
        "\n",
        "- **candidate_count:** This controls the number of responses that will be generated for a single prompt. Default value is 1. Increasing this will generate more text responses. This increase the resource usage.\n",
        "- **stop_sequence:** It allows to specify a list of strings that will act as stopsigns for the model.\n",
        "- **max_output_tokens:** This is the maximum number of tokens the model will generate in the response.\n",
        "- **temperature:** It act as a control knob that influences the randomness of the model's output. A higher temperature value will result in a more varied and creative response. Lower values would be more effective in returning predictable results with an LLM.\n",
        "- **top_p:** Range from [0.0, 1.0]. This is also known a **nucleus sampling**. The LLM only considers the next word options that cumulatively add up to a probability of reaching or exceeding the `top_p` value. A higher value will create looser threshold. This will allow the model to consider a wider range of probable options while still prioritizing the most likely ones. A lower `top_p` value will create a stricter threshold, leading to less diverse and more predictable outputs.\n",
        "- **top_k:** This parameter limits the number of possible next words to the `k` most probable options based on the probability distribution. A lower `k` value restricts the selection to a smaller pool of the most likely words, leading to less diverse and more predictable outputs.\n",
        "\n",
        "Both `top_p` and `top_k` works in conjunction with the `temperature` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7389c3fa-5e03-435c-87c0-e044f16e5f9d",
      "metadata": {
        "id": "7389c3fa-5e03-435c-87c0-e044f16e5f9d",
        "outputId": "4e49d01c-4d68-4777-9061-b700d02430c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Feature Selection: The Art of Choosing the Right Features for Your Model\n",
            "\n",
            "Feature selection is a crucial step in data science, particularly in machine learning, where it plays a vital role in building efficient and accurate models. It's the process of **identifying and selecting the most relevant features from a dataset** for building a predictive model. By focusing on the most informative features, we achieve several benefits:\n",
            "\n",
            "**Why Feature Selection Matters:**\n",
            "\n",
            "* **Improved Model Performance:** Irrelevant features can introduce noise and bias, leading to overfitting and poor generalization on unseen data. Removing them enhances the model's accuracy and ability to predict future outcomes.\n",
            "* **Reduced Complexity:** Fewer features simplify the model, making it easier to understand, interpret, and deploy. This also reduces computational time and resources needed for training and prediction.\n",
            "* **Enhanced Interpretability:** By selecting the most influential features, we gain insights into the underlying relationships within the data, providing valuable information for decision-making.\n",
            "* **Reduced Data Dimensionality:** High-dimensional datasets pose challenges for model building. Feature selection helps manage this by reducing the number of features, leading to better performance and faster training.\n",
            "\n",
            "**Types of Feature Selection Methods:**\n",
            "\n",
            "There are various techniques for feature selection, which can be\n"
          ]
        }
      ],
      "source": [
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# Setting our parameters\n",
        "custom_config = genai.types.GenerationConfig(max_output_tokens=256, temperature=1.0)\n",
        "\n",
        "user_prompt = \"\"\"What is feature selection in data science? Explain in detail.\"\"\"\n",
        "\n",
        "# Passing our custom parameters to the generate_content method\n",
        "response = model.generate_content(user_prompt, generation_config=custom_config)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0759e05-8add-4642-9597-95aac41cb3ad",
      "metadata": {
        "id": "c0759e05-8add-4642-9597-95aac41cb3ad",
        "outputId": "5684d213-bd68-4755-9448-f93f7b2b5831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Feature Selection: The Art of Choosing the Right Variables\n",
            "\n",
            "In data science, feature selection is a crucial process that involves **identifying and selecting the most relevant features (variables) from a dataset** for use in building a predictive model. It's like choosing the right ingredients for a recipe – the wrong ones can ruin the dish, while the right ones create a masterpiece.\n",
            "\n",
            "**Why is Feature Selection Important?**\n",
            "\n",
            "* **Improved Model Performance:** Irrelevant or redundant features can introduce noise and complexity, hindering model accuracy and generalization. Selecting the right features can lead to simpler, more interpretable, and more accurate models.\n",
            "* **Reduced Overfitting:** Overfitting occurs when a model learns the training data too well, failing to generalize to unseen data. Feature selection helps prevent this by reducing the number of features, thus reducing the model's complexity.\n",
            "* **Faster Training and Deployment:** Fewer features mean less data to process, leading to faster model training and deployment times.\n",
            "* **Enhanced Interpretability:** Models with fewer features are easier to understand and interpret, making it easier to identify the key factors driving predictions.\n",
            "\n",
            "**Types of Feature Selection Techniques:**\n",
            "\n",
            "There are three main categories of feature selection techniques:\n",
            "\n",
            "1. **Filter Methods:** These methods evaluate features independently based on their intrinsic properties, without considering the model. Examples include:\n",
            "    * **Univariate Feature Selection:** Measures the relationship between each feature and the target variable using statistical tests like chi-squared, ANOVA, or correlation coefficients.\n",
            "    * **Variance Threshold:** Removes features with low variance, as they are unlikely to contribute significantly to the model.\n",
            "2. **Wrapper Methods:** These methods use a specific machine learning model to evaluate the performance of different feature subsets. They are more computationally expensive but can lead to better results. Examples include:\n",
            "    * **Recursive Feature Elimination (RFE):** Iteratively removes features based on their importance scores until a desired number of features remains.\n",
            "    * **Forward Feature Selection:** Starts with an empty set of features and iteratively adds the feature that improves the model performance the most.\n",
            "    * **Backward Feature Selection:** Starts with all features and iteratively removes the feature that has the least impact on the model performance.\n",
            "3. **Embedded Methods:** These methods incorporate feature selection as part of the model training process. They are often more efficient and can provide insights into feature importance. Examples include:\n",
            "    * **Lasso Regression:** A linear regression model that uses L1 regularization to shrink the coefficients of less important features towards zero.\n",
            "    * **Decision Trees:** These models inherently perform feature selection by splitting nodes based on the most informative features.\n",
            "    * **Random Forest:** An ensemble of decision trees that can be used to identify important features based on their frequency of selection across trees.\n",
            "\n",
            "**Choosing the Right Technique:**\n",
            "\n",
            "The best feature selection technique depends on the specific dataset, the model being used, and the desired outcome. Consider factors like:\n",
            "\n",
            "* **Data Size:** For large datasets, filter methods are often preferred due to their computational efficiency.\n",
            "* **Model Complexity:** Wrapper methods are more suitable for complex models where feature interactions are important.\n",
            "* **Interpretability:** Embedded methods can provide insights into feature importance, making them useful for interpretable models.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "Feature selection is a crucial step in building effective machine learning models. By carefully selecting the most relevant features, we can improve model performance, reduce overfitting, and enhance interpretability. Choosing the right technique depends on the specific context and requires careful consideration of the trade-offs involved.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setting our parameters\n",
        "custom_config = genai.types.GenerationConfig(temperature=0.1, top_p=0.1, top_k=32)\n",
        "\n",
        "user_prompt = \"\"\"What is feature selection in data science? Explain in detail.\"\"\"\n",
        "\n",
        "# Passing our custom parameters to the generate_content method\n",
        "response = model.generate_content(user_prompt, generation_config=custom_config)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f85b4e-9740-4a79-ac1c-ff7cdfb6a02f",
      "metadata": {
        "id": "b5f85b4e-9740-4a79-ac1c-ff7cdfb6a02f",
        "outputId": "ca18416d-6a87-47fa-d8bc-c97e3e2a16b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Feature Selection: The Art of Choosing the Right Variables\n",
            "\n",
            "In data science, feature selection is a crucial process that involves **identifying and selecting the most relevant features (variables) from a dataset** for use in building a predictive model. It's like choosing the right ingredients for a recipe – the wrong ones can ruin the dish, while the right ones create a masterpiece.\n",
            "\n",
            "**Why is Feature Selection Important?**\n",
            "\n",
            "* **Improved Model Performance:** Irrelevant or redundant features can introduce noise and complexity, hindering model accuracy and generalization. Selecting the right features can lead to simpler, more interpretable, and more accurate models.\n",
            "* **Reduced Overfitting:** Overfitting occurs when a model learns the training data too well, failing to generalize to unseen data. Feature selection helps prevent this by reducing the number of features, thus reducing the model's complexity.\n",
            "* **Faster Training and Deployment:** Fewer features mean less data to process, leading to faster model training and deployment times.\n",
            "* **Enhanced Interpretability:** Models with fewer features are easier to understand and interpret, making it easier to identify the key factors driving predictions.\n",
            "\n",
            "**Types of Feature Selection Techniques:**\n",
            "\n",
            "There are three main categories of feature selection techniques:\n",
            "\n",
            "1. **Filter Methods:** These methods evaluate features independently based on their intrinsic properties, without considering the model. Examples include:\n",
            "    * **Univariate Feature Selection:** Measures the relationship between each feature and the target variable using statistical tests like chi-squared, ANOVA, or correlation coefficients.\n",
            "    * **Information Gain:** Measures the reduction in entropy (uncertainty) when a feature is used to split the data.\n",
            "    * **Mutual Information:** Measures the dependency between two variables, capturing both linear and non-linear relationships.\n",
            "\n",
            "2. **Wrapper Methods:** These methods use a specific machine learning model to evaluate the performance of different feature subsets. They are more computationally expensive but can potentially find better feature combinations. Examples include:\n",
            "    * **Forward Selection:** Starts with an empty set of features and iteratively adds the feature that improves the model performance the most.\n",
            "    * **Backward Elimination:** Starts with all features and iteratively removes the feature that has the least impact on model performance.\n",
            "    * **Recursive Feature Elimination (RFE):** Repeatedly removes features based on their importance scores until a desired number of features remains.\n",
            "\n",
            "3. **Embedded Methods:** These methods integrate feature selection into the model training process itself. They are often more efficient and can find features that are relevant in the context of the specific model. Examples include:\n",
            "    * **Lasso Regression:** A linear regression model that uses L1 regularization to shrink the coefficients of irrelevant features to zero.\n",
            "    * **Decision Trees:** Features with higher information gain are prioritized in the tree structure, effectively selecting the most relevant features.\n",
            "    * **Random Forest:** Features with higher importance scores (based on how often they are used in the decision trees) are considered more relevant.\n",
            "\n",
            "**Choosing the Right Technique:**\n",
            "\n",
            "The best feature selection technique depends on the specific dataset, the model being used, and the desired trade-off between performance, interpretability, and computational cost.\n",
            "\n",
            "**Considerations for Feature Selection:**\n",
            "\n",
            "* **Data Type:** Different techniques are suitable for different data types (e.g., numerical, categorical).\n",
            "* **Data Size:** Large datasets may require more efficient techniques.\n",
            "* **Model Complexity:** Complex models may benefit from more sophisticated feature selection methods.\n",
            "* **Domain Expertise:** Understanding the underlying domain can guide feature selection by identifying potentially relevant features.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "Feature selection is a crucial step in building effective machine learning models. By carefully selecting the most relevant features, we can improve model performance, reduce overfitting, and enhance interpretability. Choosing the right technique and considering the specific context of the problem are essential for achieving optimal results.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setting our parameters\n",
        "custom_config = genai.types.GenerationConfig(temperature=0.9, top_p=0.1, top_k=32)\n",
        "\n",
        "user_prompt = \"\"\"What is feature selection in data science? Explain in detail.\"\"\"\n",
        "\n",
        "# Passing our custom parameters to the generate_content method\n",
        "response = model.generate_content(user_prompt, generation_config=custom_config)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f3edec-a6a8-43aa-b3bf-5b22327274e6",
      "metadata": {
        "id": "f0f3edec-a6a8-43aa-b3bf-5b22327274e6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}